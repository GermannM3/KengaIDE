[package]
name = "local_provider"
version = "0.1.0"
edition = "2021"
description = "Offline LocalProvider для GigaChat Ultra (GGUF, llama.cpp)"

[dependencies]
ai_providers = { path = "../ai_providers" }
async-stream = "0.3"
async-trait = "0.1"
dirs = "5"
futures-util = "0.3"
llama-cpp-2 = { version = "0.1", features = ["sampler"] }
reqwest = { version = "0.12", default-features = false, features = ["rustls-tls", "stream", "json"] }
serde = { version = "1", features = ["derive"] }
sha2 = "0.10"
serde_json = "1"
thiserror = "1"
tokio = { version = "1", features = ["fs", "io-util", "sync", "time"] }
tracing = "0.1"
zip = "1"

[target.'cfg(unix)'.dependencies]
sysinfo = "0.30"

[target.'cfg(windows)'.dependencies]
sysinfo = "0.30"
